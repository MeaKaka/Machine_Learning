#### 视频讲解大纲

CH1 简介机器学习（监督学习、无监督学习）</br>
CH2 监督学习Ⅰ——模型一览(损失函数、梯度下降、针对线性回归)</br>
CH3 矩阵与向量</br>
CH4 多重特征 梯度下降算法的具体实例</br>
CH5 数据</br>
CH6 监督学习Ⅱ——分类问题（逻辑回归，高级优化算法，逻辑回归解决多分类问题）</br>
CH7 过拟合问题（正则化，加入惩罚项）</br>
CH8 监督学习Ⅲ——非线性分类器（即神经网络，以手写体识别为例）</br>
CH9 为神经网络拟合参数 （损失函数、梯度检测、随机初始化）</br>
CH10 对已有模型进行优化（加数据还是加特征、交叉验证、学习曲线、错误率与过拟合的关系）</br>
CH11 如何构建一个好的机器学习系统（朴素贝叶斯）</br>
CH12 监督学习Ⅳ——支持向量机SVM（损失函数，kernels）</br>
CH13 无监督学习问题Ⅰ——聚类（K-means算法、优化目标、K的值-肘部法则）</br>
CH14 无监督学习问题Ⅱ——降维（数据压缩，PCA，与线性回归的对比）</br>
CH15 异常检测（高斯分布、如何评价异常检测算法、与监督学习的对比）</br>
CH16 推荐系统（机器学习应用示例，以电影推荐为例，协同过滤算法）</br>
CH17 大数据下的机器学习（SGD随机梯度下降、Mini-batch小批次随机梯度下降、在线学习、Map Reduce）</br>
CH18 照片OCR（流水线、滑动窗口、人工生成数据）</br>
CH19 总结、致谢与道别</br>


#### 单元重点展开
##### 梯度下降
梯度下降的目的是为了使J(θ)最小,J(θ)为已有数据集中所有x的预测值h(x)和实际值y的差值，
```
\frac{1}{2m}\sum(h(x_j)-y_j)^2
```
此时，针对每一个θ的修改值应当为，

```
θ_0=θ_0-a\frac{α}{αθ_0}(J(θ_0,θ_1))
```
可以转化为，

```
θ_0=θ_0-a\frac{1}{m}\sum(h(x_j)-y_j)
```
针对θ1及其他θi的情况，可以书写为，
```
θ_i=θ_i-a\frac{1}{m}\sum(h(x_j)-y_j)(x_i)
```
有N个特征值的情况时，h(x)可以看成以下形式，
```
h(x)=θ_0+θ_1X_1+\cdots+θ_nX_n
```
通过求导，确定移动方向，最终使得曲线下降到最低点，最终实现h(x)和y的拟合，以达到预测的目的。

![梯度下降图像](C:\Users\胡晓慧\Desktop\Study\机器学习\笔记\img\吴恩达梯度下降.PNG)

Tips:

​	梯度下降中需要注意的是要考虑特征缩放特别是当特征向量多的时候。特征缩放指的是θi的取值相似。

​	学习速率的确定一般是 ...0.001  0.01  0.1  1...(x3)。

##### 逻辑回归
使用逻辑回归大多是进行分类，classification，常见的是通过Sigmoid函数进行分类。
![sigmod函数图像](C:\Users\胡晓慧\Desktop\Study\机器学习\笔记\img\sigmod函数.png)

通过Sigmoid函数的图像可以看出，当输入x小于0的时候，Sigmoid函数的输出值小于0.5；当输入x大于0的时候，Sigmoid函数的输出值大于0.5。因此，当输出值小于0.5的时候预测为0；大于等于0.5时预测为1。

通过多元多项式构造复杂的决策边界更好的进行数据的拟合，但是一定要考虑过拟合问题，这是多元多项式是Sigmoid函数的输入。

对于分类问题的损失函数cost(hθ(x),y)，y只有0、1两个取值，因此，可以通过log函数简化为只与hθ(x)有关的函数。

当y=1时，
```
cost(h_θ(x),y) = -log(h_θ(x))
```
当y=0时，
```
cost(h_θ(x),y) = -log(1-h_θ(x))
```
最终，cost()函数可以看作是，
```
cost(h_θ(x),y) = -ylog(h_θ(x))+(1-y)log(1-h_θ(x))
```


##### 神经网络

利用神经网络构建模型的基本流程：

1. 构建网络（确定输入、输出、隐藏单元）
2. 随机初始化权重值
3. 前向传播（激励）
4. 计算损失函数
5. 反向传播
6. 梯度检测
7. 利用梯度下降算法等进行优化修改权重值重复3-7

输出是one hot编码，可以利用已有的工具进行神经网络损失函数的计算。

梯度检测是为了防止反向传播可能出现错误，目的是证明计算的梯度是正确的。利用双侧查分的方式，
```
\frac{J(θ+ε)+J(θ+ε)}{2ε}
```


##### 已有模型优化

已有模型优化的六种方式：

1. 增加数据量
2. 减少特征集合规模
3. 增加特征量
4. 增加多项式特征
5. 增大λ的值（λ是利用正则化处理过拟合问题中加入惩罚项的参数）
6. 减少λ的值

过拟合问题是指在你和训练数据的时候用到的特征变量过多，使得已有函数对训练数据拟合效果非常好但是无法泛化到新的数据中。这时可以利用正则化的方式对损失函数进行更改，
```
J(θ)=\frac{1}{2m}[\sum(h(x_i)-y_i)^2-λ\sumθ_j^2]
```
关于数据拟合核心就是两个问题，一是拟合不充分；而是过拟合。针对拟合不充分的问题可以增加特征量、利用增加多项式特征、减少λ的值等方法来解决；而针对过拟合问题可以通过增加测试数据的数量、减少特征集合的规模、增大惩罚参数λ的值来解决。这样问题就变成了如何判断模型与数据是否处于欠拟合或过拟合状态，针对这个问题可以利用交叉验证法进行验证。将数据以一定的比例（一般是6：2：2）分为训练集、验证集和测试集，结合测试集和验证集的数据量/错误率图像判断当前模型状态。

![过拟合与欠拟合图像](C:\Users\胡晓慧\Desktop\Study\机器学习\笔记\img\过拟合与欠拟合.png)

此外，训练集和交叉验证集构成的学习曲线，也可以用来判断当前学习的状态，以确定是否需要增加训练样本。

![学习曲线图像](C:\Users\胡晓慧\Desktop\Study\机器学习\笔记\img\学习曲线.png)

##### 朴素贝叶斯

当出现正负样本比例不均衡的情况时，若要考虑分类的精准度，则要涉及准确度、召回率等概念。

准确度，被正确识别的正类别样本/所有被预测为正类别的样本，指的是被判断为正的样本中正样本的比例；召回率（recall），被正确识别的正类别样本/所有正类别样本，指的是所有正类别样本中能被正确识别的概率。

##### SVM

支持向量机的实质是将一段连续的曲线转变为两段折线。

##### 聚类——K-means

1. 随机选取聚类中心K
2. 进行样本聚类
3. 更新聚类中心的位置（求所有聚类到此中心的样本的中心更改为聚类的K个中心之一）
4. 重复2、3直到聚类中心的位置不在更改

如何选取K的值以及K个聚类中心的位置是问题的关键，一般来说，聚类中心的位置是随机初始化，不同的中心的位置导致的结果可能大不相同。至于K的大小一般取2-10之间，通过多次随机去寻找效果最好的情况（肘部法则）。

##### 降维——PCA

principal components analysis主成分分析，目的是为了选择一个低维投影平面使得投影误差最小。一般来说，不到万不得已的情况下不使用PCA算法。

1. 数据预处理
2. 矩阵降维

对比和线性回归的差别，在于样本和预测平面的距离的求法，前者只与y有关，后者和x、y都有关。

##### 异常检测

数据的分布符合高斯分布，因此一般利用数据的高斯分布去判断数据是否异常。

![高斯分布图像](C:\Users\胡晓慧\Desktop\Study\机器学习\笔记\img\高斯分布.png)

可以通过log()函数将非高斯分布的数据转化为符合高斯分布的数据。

对比监督学习，异常监测主要运用的场景是正例少、负例多的情况。

##### 协同过滤

不基于内容的推荐算法，通过用户的评价自行学习需要的特征，根据相似用户的喜好进行作品推荐。

##### SGD随机梯度下降

stochastic gradient descent随机梯度下降，核心在随机上，对比普通的梯度下降（batch gradient descent）在于用于计算梯度的数据是否是全部的样本数据。

1. 随机打乱数据
2. 计算针对第i个样本的梯度变化
3. 对m个样本都进行操作2
4. 对操作2、3重复n次，n的取值范围为[1,10]

此外还有mini-batch gradient descent小批次的梯度下降算法，计算梯度的数据是整个样本中的小批次数据。
